"""Measurement history handling utilities."""

import csv
import datetime
import fnmatch
import json
import logging
import os
import re
import shutil
from glob import glob
from typing import List

import geist  # type: ignore
import jmespath  # type: ignore
from geist.system import system_report  # type: ignore

_LOGGER = logging.getLogger(__name__)

DEFAULT_DEVICE = "zeus"

ArchiveData = List[dict]
SYS_REPORT = system_report()
if SYS_REPORT == {}:
    _LOGGER.error(
        """Could not retrieve hardware information.\
        Fall back to default: %s""",
        DEFAULT_DEVICE,
    )
    SYS_REPORT["hardware"] = {"name": DEFAULT_DEVICE}


HEADER_STRINGS = ["start date", "measurement ID", "intact cells/ml"]
SUMMARY_FIELDS = {
    "start_date": "status.startedAt",
    "col_id": "id",
    "bacteria_con": "display.concentration.sampleVolume.bacteria",
}

# Additional columns for zeus devices
if SYS_REPORT["hardware"]["name"] == "zeus":
    SUMMARY_FIELDS["particle_con"] = "display.concentration.sampleVolume.nonbacteria"
    HEADER_STRINGS.append("particles/ml")


class FrogError(Exception):
    """frog service error instance."""


def _dump_measurements(sink: str, source: str) -> bool:
    """Copy all .csv files from source/ to sink/"""

    sink_media = sink.replace("dev", "media")

    # create new folder
    sink_base_name = "sbt_data_"
    now = datetime.datetime.now()
    date_str = now.strftime("%Y_%m_%d_%H_%M_%S")

    sink_path = sink_media + sink_base_name + date_str

    os.mkdir(sink_path)

    meas_files = glob(source + "*.csv")

    for file in meas_files:
        shutil.copy2(file, sink_path)

    return True
    # emit progress
    # emit success, fail


SUMMARY_FILE = "measurement_summary.csv"


def gen_csv_summary(sink: str, source: str = "/srv/baxter/storage/") -> None:
    """Create summary csv file from all .json files at source"""
    file_list = get_json_list(source)
    _LOGGER.debug("JSON files: %d", len(file_list))
    data = read_archive_field(
        file_list, SUMMARY_FIELDS, skip_invalid_files=True, include_file_name=False
    )

    _LOGGER.debug("format data entries")
    # sort the data by multiple keys
    data.sort(key=lambda row: row["col_id"])
    data.sort(
        key=lambda row: datetime.datetime.strptime(
            row["start_date"], "%Y-%m-%d %H:%M:%S.%f"
        ),
        reverse=True,
    )
    _format_data(data)

    _LOGGER.info("Writting summary csv file...")
    try:
        with open(sink + SUMMARY_FILE, "w", newline="") as csvfile:
            sumwriter = csv.writer(
                csvfile, delimiter=",", quotechar="|", quoting=csv.QUOTE_MINIMAL
            )
            try:
                sumwriter.writerow(HEADER_STRINGS)
                for entry in data:
                    sumwriter.writerow(entry.values())
            except csv.Error as err:
                raise FrogError("Could not write CSV row.") from err
    except OSError as err:
        raise FrogError(f"Could not create file '{SUMMARY_FILE}'") from err


_MEXP = re.compile("[a-z0-9A-Z-_]*.json")


def get_json_list(target_dir: str = "/srv/baxter/storage/") -> list:
    """Return the list of (json) measurement files."""
    try:
        files = os.listdir(target_dir)
    except FileNotFoundError as exc:
        raise FrogError("File not found.") from exc
    files = [target_dir + file for file in files if _MEXP.match(file)]
    return files


def read_archive_field(
    archive_list: list,
    fields: dict,
    *,
    skip_invalid_files: bool = False,
    return_after_n=0,
    include_file_name=True,
) -> ArchiveData:
    """Read FIELDS of records defined by ARCHIVE_LIST. Data is returned as a
    list of dictionaries.

    Fields is dict in the form of key:path_to_json_value e.g.:
    "bacteria_con":"analysis.concentration.sampleVolume"
    """
    out = []
    for record in archive_list:
        try:
            with open(record) as cfile:
                try:
                    data = json.load(cfile)
                except json.JSONDecodeError as err:
                    if skip_invalid_files:
                        continue
                    raise FrogError(f"Could not parse JSON of file '{record}'") from err
                try:
                    # HACK: force reading messages field, to skip errored files
                    messages = jmespath.search("messages", data)
                    tmp = {k: jmespath.search(v, data) for (k, v) in fields.items()}
                    # HACK: dash history screen depends on file_name to open the records
                    if include_file_name:
                        tmp["file_name"] = record
                except KeyError as err:
                    if skip_invalid_files:
                        continue
                    raise FrogError(f"Missing entries in '{record}'") from err
                # HACK: Skip files that have an empty messages field.
                # Errored measurements have a non empty messages field.
                if not messages:
                    out.append(tmp)
            if (return_after_n > 0) and len(out) > return_after_n:
                break
        except OSError as err:
            raise FrogError(f"Could not open file '{record}'") from err
    return out


def get_archive_record(
    file_name: str, target_dir: str = "/srv/baxter/storage/"
) -> dict:

    """Return data of a single measurement record."""
    if os.path.isfile(file_name):
        target_dir = ""
    try:
        with open(target_dir + file_name) as record:
            try:
                data = json.load(record)
            except json.decoder.JSONDecodeError as err:
                raise FrogError(f"Could not read json file '{file_name}'.") from err
            except OSError as err:
                raise FrogError(f"Could not open file '{file_name}'.") from err

    except FileNotFoundError:
        _LOGGER.error("Could not open measurement record.")
        data = {}
    return geist.basic.keys_to_snake_case(data)


def _format_data(data: ArchiveData) -> None:
    """Custom formatting of the output values."""
    for row in data:
        # format dates
        for key, val in row.items():
            if key == "start_date":
                date_object = datetime.datetime.strptime(val, "%Y-%m-%d %H:%M:%S.%f")
                row[key] = date_object.strftime("%Y-%m-%d %H:%M")
            elif key in ["bacteria_con", "particle_con"]:
                if isinstance(val, str):
                    # remove unicode thin space
                    row[key] = val.replace("\u2009", " ")
                    # HACK: remove unicode symbol â‰¥ (u2265)
                    row[key] = row[key].replace("\u2265", ">=")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Dump csv files to removable drive.")
    parser.add_argument(
        "sink",
        metavar="copy/measurements/here/",
        type=str,
        help="Path where to copy the measurements",
    )
    parser.add_argument(
        "--source",
        dest="source",
        default="/srv/baxter/storage/",
        help="Path to measurement folder",
    )

    args = parser.parse_args()
    _dump_measurements(args.sink, args.source)
